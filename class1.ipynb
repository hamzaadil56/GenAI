{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ : bool = load_dotenv() # read local .env file\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ : bool = load_dotenv() # read local .env file\n",
    "client : OpenAI = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1 equals 2.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from openai.types.chat.chat_completion import ChatCompletion\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ : bool = load_dotenv() # read local .env file\n",
    "client : OpenAI = OpenAI()\n",
    "def chat_completion(prompt : str )-> str:\n",
    " response : ChatCompletion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "#  print(response)\n",
    "#  print(response.choices[0].message.content)\n",
    " return response.choices[0].message.content\n",
    "\n",
    "chat_completion(\"what is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Quantum computing is like the superhero of the computing world, wielding powers that can harness the mind-boggling phenomena of quantum mechanics to solve problems way faster than a regular computer. \n",
      "\n",
      "In the quantum world, instead of the usual bits used by traditional computers, which are either 0 or 1, we have quantum bits, or qubits. Qubits can exist in multiple states at the same time, a mind-bending property known as superposition. This allows quantum computers to process a vast amount of information simultaneously.\n",
      "\n",
      "But that's not all! Qubits also have a property called entanglement, where the state of one qubit is intrinsically linked to the state of another, no matter the distance between them. This means that quantum computers can perform complex tasks with incredible speed and efficiency.\n",
      "\n",
      "Imagine trying to solve a maze with countless paths using a regular computer - it would have to painstakingly check each possibility one by one. In contrast, a quantum computer could explore all the paths at once, thanks to its superposition and entanglement powers, and swiftly find the right solution in a fraction of the time.\n",
      "\n",
      "Though quantum computing is still in its early stages, scientists are excited about its potential to revolutionize fields such as cryptography, drug discovery, weather forecasting, and beyond. It's like the dawn of a new era where our computers can tap into the mind-bending wonders of quantum mechanics to tackle the toughest challenges in lightning-fast fashion!\n"
     ]
    }
   ],
   "source": [
    "from openai.types.chat.chat_completion import ChatCompletion\n",
    "\n",
    "def chat_completion()->str:\n",
    "  completion : ChatCompletion = client.chat.completions.create(\n",
    "    model  = \"gpt-3.5-turbo-1106\",\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are a science teacher, skilled in explaining science concepts with creative flair.\"},\n",
    "      {\"role\": \"user\", \"content\": \"Explain me about quantum computing.\"}\n",
    "    ]\n",
    "  )\n",
    "\n",
    "  return completion.choices[0].message.content\n",
    "\n",
    "print(chat_completion())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This\n",
      " is\n",
      " a\n",
      " test\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n",
    "    stream=True,\n",
    ")\n",
    "for part in stream:\n",
    "    print(part.choices[0].delta.content or \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hamza\\Documents\\CODING FILES\\Generative AI\\gENai\\class1.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hamza/Documents/CODING%20FILES/Generative%20AI/gENai/class1.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     chat_completion \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m client\u001b[39m.\u001b[39mchat\u001b[39m.\u001b[39mcompletions\u001b[39m.\u001b[39mcreate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hamza/Documents/CODING%20FILES/Generative%20AI/gENai/class1.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         messages\u001b[39m=\u001b[39m[\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hamza/Documents/CODING%20FILES/Generative%20AI/gENai/class1.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m             {\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hamza/Documents/CODING%20FILES/Generative%20AI/gENai/class1.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hamza/Documents/CODING%20FILES/Generative%20AI/gENai/class1.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hamza/Documents/CODING%20FILES/Generative%20AI/gENai/class1.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m chat_completion\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hamza/Documents/CODING%20FILES/Generative%20AI/gENai/class1.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m asyncio\u001b[39m.\u001b[39;49mrun(main())\n",
      "File \u001b[1;32mc:\\Users\\hamza\\anaconda3\\envs\\py311\\Lib\\asyncio\\runners.py:186\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \n\u001b[0;32m    163\u001b[0m \u001b[39mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[39m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[39mif\u001b[39;00m events\u001b[39m.\u001b[39m_get_running_loop() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[39m# fail fast with short traceback\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    189\u001b[0m \u001b[39mwith\u001b[39;00m Runner(debug\u001b[39m=\u001b[39mdebug) \u001b[39mas\u001b[39;00m runner:\n\u001b[0;32m    190\u001b[0m     \u001b[39mreturn\u001b[39;00m runner\u001b[39m.\u001b[39mrun(main)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "async def main() -> None:\n",
    "    chat_completion = await client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Say this is a test\",\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "\n",
    "\n",
    "    )\n",
    "    return chat_completion\n",
    "\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
